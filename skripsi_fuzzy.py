# -*- coding: utf-8 -*-
"""Skripsi Fuzzy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGFezY6mwWRkrHtOlf1KauC87YTyRMDF
"""

import importlib
import subprocess
import sys

def ensure_package(import_name, pip_name=None):
    pip_name = pip_name or import_name
    try:
        importlib.import_module(import_name)
        print(f"{pip_name} already installed")
    except ImportError:
        print(f"Installing {pip_name}...")
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", pip_name],
            stdout=subprocess.DEVNULL
        )
        print(f"{pip_name} installed")

ensure_package("noise")
ensure_package("hkb_diamondsquare")
ensure_package("pyfastnoiselite")

"""# Global Variable"""

grid_x = 512
grid_y = 512
random_seed = 1000

biomes = [
  'Rain Forest',
  'Arid',
  'Mesic',
  'Desert',
  'Forest',
  'Grassland',
  'Void'
]

perlin_parameter_values = {
    'Curah Hujan': {
      'scale': 100,
      'octaves': 6,
      'persistence': 0.5,
      'lacunarity': 1.8
    },
    'Kesuburan Tanah': {
      'scale': 100,
      'octaves': 7,
      'persistence': 0.7,
      'lacunarity': 2.2
    },
    'Kelembapan': {
      'scale': 100,
      'octaves': 4,
      'persistence': 0.4,
      'lacunarity': 1.8
    },
    'Suhu': {
      'scale': 100,
      'octaves': 3,
      'persistence': 0.5,
      'lacunarity': 1.7
    },
}
diamond_square_parameter_values = {
    'Altitude': {
        'roughness': 0.28,
    }
}
celular_automata_values = {
    'Vegetasi': {
        'birth': 5,
        'survive_min': 1,
        'survive_max': 6,
        'steps': 2,
    }
}

parameter_ranges = {
    'Curah Hujan': {'Low': (142.5, 605.85), 'Medium': (549.1, 1417.5), 'High': (1283.45, 1837.5)},
    'Kesuburan Tanah': {'Low': (0, 40), 'Medium': (30, 70), 'High': (60, 100)},
    'Kelembapan': {'Low': (0, 40), 'Medium': (30, 70), 'High': (60, 100)},
    'Vegetasi': {'Low': (0, 0), 'Medium': (1, 1), 'High': (2, 2)},
    'Suhu': {'Low': (14.25, 19.95), 'Medium': (19, 25.2), 'High': (23.75, 31.5)},
    'Altitude': {'Low': (142.25, 823.4), 'Medium': (745.75, 1869), 'High': (1691.95, 2205)},
}

biome_characteristics = {
  'Rain Forest': {
      'Curah Hujan': 'High', 'Kesuburan Tanah': 'High', 'Kelembapan': 'High', 'Vegetasi': 'High', 'Suhu': 'Medium', 'Altitude': 'Low',
  },
  'Arid': {
      'Curah Hujan': 'Low', 'Kesuburan Tanah': 'High', 'Kelembapan': 'Low', 'Vegetasi': 'Low', 'Suhu': 'High', 'Altitude': 'Medium',
  },
  'Mesic': {
      'Curah Hujan': 'Medium', 'Kesuburan Tanah': 'Low', 'Kelembapan': 'High', 'Vegetasi': 'High', 'Suhu': 'Medium', 'Altitude': 'Medium',
  },
  'Desert': {
      'Curah Hujan': 'Low', 'Kesuburan Tanah': 'Low', 'Kelembapan': 'Low', 'Vegetasi': 'Low', 'Suhu': 'Medium', 'Altitude': 'Medium',
  },
  'Forest': {
      'Curah Hujan': 'Medium', 'Kesuburan Tanah': 'High', 'Kelembapan': 'Medium', 'Vegetasi': 'High', 'Suhu': 'Medium', 'Altitude': 'Medium',
  },
  'Grassland': {
      'Curah Hujan': 'Medium', 'Kesuburan Tanah': 'Medium', 'Kelembapan': 'Medium', 'Vegetasi': 'Medium', 'Suhu': 'Low', 'Altitude': 'High',
  },
}

"""### Generate Dataset"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import time
started_time = time.time()
samples_per_biome = 250
np.random.seed(random_seed)
attributes = list(parameter_ranges.keys())

rows = []

for biome, traits in biome_characteristics.items():
    for _ in range(samples_per_biome):
        sample = {}
        for attr, level in traits.items():
            low, high = parameter_ranges[attr][level]

            mean = (low + high) / 2.0
            std = (high - low) / 6.0

            value = np.random.normal(mean, std)
            value = np.clip(value, low, high)


            sample[attr] = value

        sample['Biome'] = biome
        rows.append(sample)

df = pd.DataFrame(rows)

df_dataset = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)


generate_dataset_elapsed_time = time.time() - started_time
print("Generate Dataset Time: ", generate_dataset_elapsed_time)
numeric_columns = df_dataset.select_dtypes(include=[np.number]).columns

for col in numeric_columns:
    plt.figure()
    plt.hist(df_dataset[col], bins=20)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

dataset_x = df_dataset[['Curah Hujan', 'Kesuburan Tanah', 'Kelembapan', 'Vegetasi', 'Suhu', 'Altitude']]
dataset_y = df_dataset['Biome']

x_train, x_validate, y_train, y_validate = train_test_split(
    dataset_x,
    dataset_y,
    test_size=0.4,
    random_state=random_seed
)

print(x_train)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

samples_per_biome = 1000
np.random.seed(random_seed + 5)
attributes = list(parameter_ranges.keys())

rows = []

for biome, traits in biome_characteristics.items():
    for _ in range(samples_per_biome):
        sample = {}
        for attr, level in traits.items():
            low, high = parameter_ranges[attr][level]

            mean = (low + high) / 2.0
            std = (high - low) / 6.0

            value = np.random.normal(mean, std)
            value = np.clip(value, low, high)

            sample[attr] = value

        sample['Biome'] = biome
        rows.append(sample)

df = pd.DataFrame(rows)

df_dataset_testing = df.sample(
    frac=1,
    random_state=random_seed + 5
).reset_index(drop=True)

print(df_dataset_testing.head(20))
print(f"\nDataset shape: {df_dataset_testing.shape}")

x_test = df_dataset_testing[
    ['Curah Hujan', 'Kesuburan Tanah', 'Kelembapan', 'Vegetasi', 'Suhu', 'Altitude']
]
y_test = df_dataset_testing['Biome']



"""## Penalty Matrix"""

value_categories = {'Low': 1, 'Medium': 2, 'High': 3}

def penalty(b1, b2):
    return sum(
        abs(
            value_categories[biome_characteristics[b1][attr]] -
            value_categories[biome_characteristics[b2][attr]]
        )
        for attr in biome_characteristics[b1]
    )

biomes = list(biome_characteristics.keys())

penalty_matrix = [
    [penalty(b1, b2) for b2 in biomes]
    for b1 in biomes
]

penalty_lookup = {
    b1: {b2: penalty(b1, b2) for b2 in biomes}
    for b1 in biomes
}

print(penalty_matrix)
print(penalty_lookup)

"""# Procedural Algorithms

## Perlin Noise
"""

import numpy as np
import matplotlib.pyplot as plt
from pyfastnoiselite.pyfastnoiselite import FastNoiseLite, NoiseType, FractalType

def generate_2d_noise(width, height, scale=10.0, octaves=6,
                      persistence=0.5, lacunarity=2.0, seed=32132133):
    print(seed)
    print(f"Width {width}, Height {height}, scale {scale}, octave {octaves}, persistence {persistence}, lacunarity {lacunarity}")

    if scale <= 0:
        raise ValueError("scale must be > 0")

    noise = FastNoiseLite(seed=int(seed))

    noise.noise_type = NoiseType.NoiseType_Perlin
    noise.frequency = 1.0 / float(scale)

    noise.fractal_type = FractalType.FractalType_FBm
    noise.fractal_octaves = int(octaves)
    noise.fractal_lacunarity = float(lacunarity)

    noise.fractal_gain = float(persistence)

    xs = np.tile(np.arange(width, dtype=np.float32), height)
    ys = np.repeat(np.arange(height, dtype=np.float32), width)
    zs = np.zeros(width * height, dtype=np.float32)

    coords = np.stack([xs, ys, zs], axis=0)
    vals = noise.gen_from_coords(coords)

    world = vals.reshape(height, width).astype(np.float32)

    mn = float(world.min())
    mx = float(world.max())
    if mx - mn < 1e-8:
        return np.zeros_like(world)
    world_normalized = (world - mn) / (mx - mn)
    return world_normalized

terrain = generate_2d_noise(grid_x, grid_y, scale=100.0, octaves=5, persistence=0.3, lacunarity=2, seed=random_seed)
plt.imshow(terrain, cmap="gray", vmin=0, vmax=1)
plt.colorbar(label="Elevation")
plt.title("Perlin Noise Terrain")
plt.show()

"""## Diamond Square"""

from hkb_diamondsquare import DiamondSquare as DS
def generate_diamond_square(shape=(129, 129), min_height=0.0, max_height=1.0, roughness=0.48, seed=None):
    hm = DS.diamond_square(shape=shape,
                           min_height=min_height,
                           max_height=max_height,
                           roughness=roughness,
                           random_seed=seed)
    return np.array(hm)


tes = generate_diamond_square((grid_x, grid_y))
print(tes)
plt.imshow(tes, cmap="gray", vmin=0, vmax=1) 
plt.colorbar(label="Elevation")
plt.title("Diamond Square Terrain")
plt.show()

"""## CA"""

def celular_automata(seed_map: np.ndarray, birth=5, survive_min=1, survive_max=6, steps=2) -> np.ndarray:
    grid = (seed_map > 0.5).astype(np.uint8)
    H, W = grid.shape
    for _ in range(steps):
        new = grid.copy()
        for y in range(H):
            for x in range(W):
                y0, y1 = max(0, y-1), min(H, y+2)
                x0, x1 = max(0, x-1), min(W, x+2)
                neigh = grid[y0:y1, x0:x1].sum() - grid[y,x]
                if grid[y,x] == 1:
                    new[y,x] = 1 if (survive_min <= neigh <= survive_max) else 0
                else:
                    new[y,x] = 1 if neigh >= birth else 0
        grid = new
    return grid.astype(float)

np.random.seed(random_seed)

seed_map = np.random.rand(128, 128)
ca_result = celular_automata(seed_map, steps=2)

plt.figure()
plt.imshow(ca_result, cmap="gray", vmin=0, vmax=1)  # try "gray", "viridis", "plasma", etc.
plt.colorbar(label="Elevation")
plt.title("CA")
plt.show()



def neighborhood_average(grid: np.ndarray) -> np.ndarray:
    H, W = grid.shape
    result = np.zeros_like(grid, dtype=float)

    for y in range(H):
        for x in range(W):
            y0, y1 = max(0, y - 1), min(H, y + 2)
            x0, x1 = max(0, x - 1), min(W, x + 2)

            window = grid[y0:y1, x0:x1]

            total = window.sum()
            neighbor_count = window.size - 1  # minus itself

            if neighbor_count == 0:
                avg = 0
            else:
                avg = total / neighbor_count

            # classification
            if avg == 0:
                result[y, x] = 0
            elif 0 < avg <= 0.5:
                result[y, x] = 1
            else:
                result[y, x] = 2

    return result
res_avg = neighborhood_average(ca_result)
unique_values, counts = np.unique(res_avg, return_counts=True)

# Print results
for value, count in zip(unique_values, counts):
    print(f"Value {value}: {count} times")
plt.figure()
plt.imshow(res_avg, cmap="gray", vmin=0, vmax=3) 
plt.colorbar(label="Elevation")
plt.title("CA (Mod)")
plt.show()

def generate_celular_automata(shape=(128, 128), birth=5, survive_min=1, survive_max=6, steps=2, seed=None):
    np.random.seed(seed)
    seed_map = np.random.rand(shape[0], shape[1])
    ca_result = celular_automata(seed_map=seed_map, birth=birth, survive_min=survive_min, survive_max=survive_max, steps=steps)
    ca_result_avg = neighborhood_average(ca_result)
    return ca_result_avg

"""# Generate Value"""

grid_parameter = {}
for parameter, value in perlin_parameter_values.items():
  grid_parameter[parameter] = generate_2d_noise(grid_x, grid_y, scale=value['scale'], octaves=value['octaves'], persistence=value['persistence'], lacunarity=value['lacunarity'], seed=random_seed)
for parameter, value in diamond_square_parameter_values.items():
  grid_parameter[parameter] = generate_diamond_square((grid_x, grid_y), value['roughness'], seed=random_seed)
for parameter, value in celular_automata_values.items():
  grid_parameter[parameter] = generate_celular_automata((grid_x, grid_y), birth=value['birth'], survive_min=value['survive_min'], survive_max=value['survive_max'], steps=value['steps'], seed=random_seed)



grid_value = [[{} for _ in range(grid_x)] for _ in range(grid_y)]
for y in range(grid_y):
  for x in range(grid_x):
    for parameter, grid in grid_parameter.items():
      if parameter in ('Kesuburan Tanah', 'Kelembapan'):
        grid_value[y][x][parameter] = (grid[y][x] * 100.0)
      elif parameter in ('Vegetasi'):
        grid_value[y][x][parameter] = grid[y][x]
      else:
        grid_value[y][x][parameter] = parameter_ranges[parameter]['Low'][0] + (grid[y][x] * (parameter_ranges[parameter]['High'][1] - parameter_ranges[parameter]['Low'][0]))



df_grid = pd.DataFrame(grid_value)
records = []
for row in df_grid.values:
    for cell in row:
        if isinstance(cell, dict):
            records.append(cell)
x_classify = pd.DataFrame(records)
move_col = ['Curah Hujan', 'Kesuburan Tanah', 'Kelembapan', 'Vegetasi', 'Suhu', 'Altitude']
x_classify = x_classify[move_col]

for parameter, grid in grid_parameter.items():
  plt.figure()
  if parameter == 'Vegetasi':
    plt.imshow(grid, cmap="gray", vmin=0, vmax=3)
  else:
    plt.imshow(grid, cmap="gray", vmin=0, vmax=1)
  plt.colorbar(label=parameter)
  plt.title(parameter)
  plt.show()

"""### CA Curve Changes (Vegetasi)"""

def ca_density_step(seed_map: np.ndarray, birth=5, survive_min=1, survive_max=6, max_steps=10):
    grid = (seed_map > 0.5).astype(np.uint8)
    H, W = grid.shape

    impacts = []
    prev_grid = grid.copy()

    for step in range(max_steps):
        new = grid.copy()
        for y in range(H):
            for x in range(W):
                y0, y1 = max(0, y-1), min(H, y+2)
                x0, x1 = max(0, x-1), min(W, x+2)
                neigh = grid[y0:y1, x0:x1].sum() - grid[y,x]

                if grid[y,x] == 1:
                    new[y,x] = 1 if (survive_min <= neigh <= survive_max) else 0
                else:
                    new[y,x] = 1 if neigh >= birth else 0

        impact = np.sum(new != prev_grid)
        impacts.append(impact)

        # print(step)
        plt.figure()
        plt.imshow(grid, cmap="gray", vmin=0, vmax=1)
        plt.colorbar(label="Elevation")
        plt.title(f"CA {step}")
        plt.show()

        prev_grid = grid.copy()
        grid = new

    max_impact_step = int(np.argmax(impacts))
    print(f"Maximum change occurred at step {max_impact_step} with {impacts[max_impact_step]} changed cells")

    plt.plot(range(len(impacts)), impacts, marker='o')
    plt.title("Impact of Changes per Step")
    plt.xlabel("Step")
    plt.ylabel("Number of Changed Cells")
    plt.show()

    return grid.astype(float), impacts, max_impact_step

np.random.seed(random_seed)
seed = np.random.rand(grid_x, grid_y)
final_grid, impacts, max_step = ca_density_step(seed)

"""# AI Model

## Fuzzy
"""

import numpy as np

def fuzzy(x, membership):
    """
    Vectorized version of fuzzy().
    x: np.array of shape (grid_y, grid_x)
    membership: dict with keys 'left', 'peak', 'right'
    Returns: np.array of same shape with membership values
    """
    left = membership['left']
    peak = membership['peak']
    right = membership['right']

    y = np.zeros_like(x, dtype=float)

    # If left is peak
    if left == peak:
        y[x <= peak] = 1.0
        mask = (x > peak) & (x < right)
        y[mask] = (right - x[mask]) / (right - peak)

    # If right is peak
    elif right == peak:
        y[x < left] = 0.0
        mask = (x >= left) & (x < peak)
        y[mask] = (x[mask] - left) / (peak - left)
        y[x >= peak] = 1.0

    # If triangular
    else:
        y[x <= left] = 0.0
        y[x >= right] = 0.0
        y[x == peak] = 1.0
        mask = (x > left) & (x < peak)
        y[mask] = (x[mask] - left) / (peak - left)
        mask = (x > peak) & (x < right)
        y[mask] = (right - x[mask]) / (right - peak)

    return y

"""### Membership"""

parameter_membership = {
    'Curah Hujan': {
        'Low': lambda inp: fuzzy(inp, {'left': 142.5, 'peak': 374.175, 'right': 605.85}),
        'Medium': lambda inp: fuzzy(inp, {'left': 549.1, 'peak': 983.3, 'right': 1417.5}),
        'High': lambda inp: fuzzy(inp, {'left': 1283.45, 'peak': 1560.475, 'right': 1837.5})
    },
    'Kesuburan Tanah': {
        'Low': lambda inp: fuzzy(inp, {'left': 0, 'peak': 20, 'right': 40}),
        'Medium': lambda inp: fuzzy(inp, {'left': 30, 'peak': 50, 'right': 70}),
        'High': lambda inp: fuzzy(inp, {'left': 60, 'peak': 80, 'right': 100})
    },
    'Kelembapan': {
        'Low': lambda inp: fuzzy(inp, {'left': 0, 'peak': 20, 'right': 40}),
        'Medium': lambda inp: fuzzy(inp, {'left': 30, 'peak': 50, 'right': 70}),
        'High': lambda inp: fuzzy(inp, {'left': 60, 'peak': 80, 'right': 100})
    },
    'Vegetasi': {
        'Low': lambda inp: fuzzy(inp, {'left': 0, 'peak': 0, 'right': 0}),
        'Medium': lambda inp: fuzzy(inp, {'left': 1, 'peak': 1, 'right': 1}),
        'High': lambda inp: fuzzy(inp, {'left': 2, 'peak': 2, 'right': 2})
    },
    'Suhu': {
        'Low': lambda inp: fuzzy(inp, {'left': 14.25, 'peak': 17.1, 'right': 19.95}),
        'Medium': lambda inp: fuzzy(inp, {'left': 19, 'peak': 22.25, 'right': 25.2}),
        'High': lambda inp: fuzzy(inp, {'left': 23.75, 'peak': 27.625, 'right': 31.5})
    },
    'Altitude': {
        'Low': lambda inp: fuzzy(inp, {'left': 142.25, 'peak': 482.825, 'right': 823.4}),
        'Medium': lambda inp: fuzzy(inp, {'left': 745.75, 'peak': 1307.375, 'right': 1869}),
        'High': lambda inp: fuzzy(inp, {'left': 1691.95, 'peak': 1948.475, 'right': 2205})
    },
}


"""### Classification Report"""

import math
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, f1_score
grid_parameter_membership_report = [[{} for _ in range(len(x_test))] for _ in range(1)]
fuzzy_grid_classified_biome_report = [['Void' for _ in range(len(x_test))] for _ in range(1)]
soft_membership = 0.001
fuzzy_grid_value_report_unnormalize = {}
for col in x_test.columns:
    values = x_test[col].to_numpy()
    fuzzy_grid_value_report_unnormalize[col] = values.reshape(1, len(x_test)).tolist()

fuzzy_grid_value_report = [[{} for _ in range(len(x_test))] for _ in range(1)]
for y in range(1):
    for x in range(len(x_test)):
        for parameter, grid in fuzzy_grid_value_report_unnormalize.items():
            fuzzy_grid_value_report[y][x][parameter] = fuzzy_grid_value_report_unnormalize[parameter][y][x]

grid_value_np = {}
for param in parameter_membership.keys():
    arr = np.zeros((1, len(x_test)))
    for y in range(1):
        for x in range(len(x_test)):
            arr[y, x] = fuzzy_grid_value_report[y][x][param]
    grid_value_np[param] = arr

grid_parameter_membership_np = {}
for param, levels in parameter_membership.items():
    grid_parameter_membership_np[param] = {}
    for level, func in levels.items():
      grid_parameter_membership_np[param][level] = func(grid_value_np[param])

biome_probabilities = {}
for biome, params in biome_characteristics.items():
    arrays = []
    for param, level in params.items():
        membership_array = grid_parameter_membership_np[param][level]
        membership_array = np.where(membership_array == 0, soft_membership, membership_array)
        arrays.append(membership_array)
    biome_probabilities[biome] = np.prod(np.stack(arrays, axis=0), axis=0)

all_probs = np.stack(list(biome_probabilities.values()), axis=-1)
total = np.sum(all_probs, axis=-1, keepdims=True)
all_probs = np.where(total > 0, all_probs / total * 100, all_probs)

biome_names = list(biome_probabilities.keys())
fuzzy_grid_classified_biome = np.array(biome_names)[np.argmax(all_probs, axis=-1)]

fuzzy_grid_classified_biome_report = fuzzy_grid_classified_biome[0]

print("Predicted Biomes:\n", fuzzy_grid_classified_biome_report)
print("Accuracy:", accuracy_score(y_test, fuzzy_grid_classified_biome_report))
print(classification_report(
    y_test,
    fuzzy_grid_classified_biome_report,
    digits=6
))

precision = precision_score(
    y_test,
    fuzzy_grid_classified_biome_report,
    average='weighted'
)

recall = recall_score(
    y_test,
    fuzzy_grid_classified_biome_report,
    average='weighted'
)

f1 = f1_score(
    y_test,
    fuzzy_grid_classified_biome_report,
    average='macro'
)

cm = confusion_matrix(y_test, fuzzy_grid_classified_biome_report)

print("Precision:", precision)
print("Recall:", recall)
print("F1:", f1)
print("Confusion Matrix:\n", cm)

"""### Classification Map"""

import time

start_time = time.time()
grid_value_np = {}
for param in parameter_membership.keys():
    arr = np.zeros((grid_y, grid_x))
    for y in range(grid_y):
        for x in range(grid_x):
            arr[y, x] = grid_value[y][x][param]
    grid_value_np[param] = arr

grid_parameter_membership_np = {}
for param, levels in parameter_membership.items():
    grid_parameter_membership_np[param] = {}
    for level, func in levels.items():
        grid_parameter_membership_np[param][level] = func(grid_value_np[param])

soft_membership = 0.001
biome_probabilities = {}

for biome, params in biome_characteristics.items():
    arrays = []
    for param, level in params.items():
        membership_array = grid_parameter_membership_np[param][level]
        membership_array = np.where(membership_array == 0, soft_membership, membership_array)
        arrays.append(membership_array)

    # same_value_mask = np.all(arrays == arrays[0], axis=0)

    # ys, xs = np.where(same_value_mask)

    # for y, x in zip(ys, xs):
    #   print(f"y={y}, x={x} â†’ SAMA")
    # for membership_arrays in arrays:
    #   for membership in membership_arrays:
    #     print(membership)
    #     if np.all(membership == membership.flat[0]):
    #       print("TES")
    biome_probabilities[biome] = np.prod(np.stack(arrays, axis=0), axis=0)



all_probs = np.stack(list(biome_probabilities.values()), axis=-1)
total = np.sum(all_probs, axis=-1, keepdims=True) # ubah probabilitas jadi persentase
all_probs = np.where(total > 0, all_probs / total * 100, all_probs)
# # Argmax untuk mendapatkan biome names
biome_names = list(biome_probabilities.keys())
fuzzy_grid_classified_biome = np.array(biome_names)[np.argmax(all_probs, axis=-1)]
end_time = time.time()
fuzzy_elapsed_time = end_time - start_time
print("Classification Map Time: ", fuzzy_elapsed_time)

"""### Biome Transition Report"""

from collections import Counter
print(fuzzy_grid_classified_biome)

def get_neighbors(grid, y, x):
    neighbors = []

    # Loop through 8 surrounding cells
    for dy in [-1, 0, 1]:
        for dx in [-1, 0, 1]:
            if dy == 0 and dx == 0:
                continue  # skip itself

            ny, nx = y + dy, x + dx

            if 0 <= ny < grid_y and 0 <= nx < grid_x:
              neighbors.append(str(grid[ny][nx]))

    return neighbors

counter_grid = [[None for _ in range(grid_x)] for _ in range(grid_y)]

for y in range(grid_y):
    for x in range(grid_x):
      neighbors = get_neighbors(fuzzy_grid_classified_biome, y, x)
      counter_grid[y][x] = dict(Counter(neighbors))

print(penalty_lookup)
print(counter_grid)
penalty_grid = [[0 for _ in range(grid_x)] for _ in range(grid_y)]
penalty_average_grid = [[0 for _ in range(grid_x)] for _ in range(grid_y)]
penalty_total = 0
penalty_average_total = 0
for y in range(grid_y):
    for x in range(grid_x):
      total_neighbour = 0
      for biome, counter in counter_grid[y][x].items():
          classified_biome = fuzzy_grid_classified_biome[y][x]
          penalty = counter * penalty_lookup[classified_biome][biome]
          total_neighbour += counter
          penalty_grid[y][x] += penalty
          penalty_total += penalty
      max_categorial_score = 2
      total_parameter_biome = len(biome_characteristics.items())
      average_score_divider = total_neighbour * max_categorial_score * total_parameter_biome
      penalty_average_grid[y][x] = penalty_grid[y][x] / average_score_divider
      penalty_average_total += penalty_average_grid[y][x]

print(penalty_grid)
print(penalty_average_grid)
print(len(biome_characteristics.items()))
print(penalty_total)
print(penalty_average_total)
print(f"Global Penalty Average: {penalty_average_total / (grid_x * grid_y) * 100}%")
print(f"Global Smoothness: {100 - (penalty_average_total / (grid_x * grid_y) * 100)}%")

# Morans I

biome_to_num = {b: i for i, b in enumerate(biome_characteristics.keys())}
numeric_grid = np.vectorize(lambda x: biome_to_num[x])(fuzzy_grid_classified_biome)

def get_neighbors_coord(H, W, y, x):
    for ny in range(max(0, y-1), min(H, y+2)):
        for nx in range(max(0, x-1), min(W, x+2)):
            if (ny, nx) != (y, x):
                yield ny, nx

def compute_morans_I(numeric_grid):
    n = grid_y * grid_x
    values = numeric_grid.flatten()
    mean_val = values.mean()

    w_sum = 0
    num = 0
    denom = ((values - mean_val)**2).sum()

    for y in range(grid_y):
        for x in range(grid_x):
            v1 = numeric_grid[y][x]
            for ny, nx in get_neighbors_coord(grid_y, grid_x, y, x):
                v2 = numeric_grid[ny][nx]
                num += (v1 - mean_val) * (v2 - mean_val)
                w_sum += 1

    I = (n / w_sum) * (num / denom)
    return I

print(f"Morans I: {compute_morans_I(numeric_grid)}")

plt.imshow(penalty_average_grid, cmap="hot_r", interpolation="nearest")
plt.colorbar(label="Penalty Average")
plt.title("Biome Transition Smoothness Heatmap Fuzzy")
plt.show()


"""## KNN

### Classification Report
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, f1_score
import time

start_time = time.time()
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.transform(x_test)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

y_pred_knn = knn.predict(X_test_scaled)
print(len(y_pred_knn))
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Classification Report:\n")
print(classification_report(
    y_test,
    y_pred_knn,
    digits=6
))

precision = precision_score(
    y_test,
    y_pred_knn,
    average='weighted'
)

recall = recall_score(
    y_test,
    y_pred_knn,
    average='weighted'
)

f1 = f1_score(
    y_test,
    y_pred_knn,
    average='macro'
)

cm = confusion_matrix(y_test, y_pred_knn)

print("Precision:", precision)
print("Recall:", recall)
print("F1:", f1)
print("Confusion Matrix:\n", cm)
knn_elapsed_time = time.time() - start_time
print("Training Time: ", knn_elapsed_time)

"""K-Fold"""

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import time

n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
fold_no = 1
acc_scores = []

for train_index, test_index in kf.split(dataset_x):
    X_train_fold, X_test_fold = dataset_x.iloc[train_index], dataset_x.iloc[test_index]
    y_train_fold, y_test_fold = dataset_y.iloc[train_index], dataset_y.iloc[test_index]

    scaler_fold = StandardScaler()
    X_train_scaled_knn = scaler_fold.fit_transform(X_train_fold).astype(np.float64)
    X_test_scaled_knn  = scaler_fold.transform(X_test_fold).astype(np.float64)

    le = LabelEncoder()
    y_train_enc_knn = le.fit_transform(y_train_fold)
    y_test_enc_knn  = le.transform(y_test_fold)

    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train_scaled_knn, y_train_enc_knn)

    y_pred_knn_enc = knn.predict(X_test_scaled_knn)
    y_pred_knn = le.inverse_transform(y_pred_knn_enc)

    acc = accuracy_score(y_test_fold, y_pred_knn)
    acc_scores.append(acc)

    print(f"Fold {fold_no}: Accuracy = {acc:.4f}")
    print(f"Classification Report Fold {fold_no}:\n", classification_report(y_test_fold, y_pred_knn))
    fold_no += 1

print(f"\nAverage Accuracy: {np.mean(acc_scores):.4f}")

"""### Elbow Method"""

from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train)
X_val_scaled = scaler.transform(x_validate)

k_values = []
accuracies = []

# Coba K dari 1 sampai 20
for k in range(1, 21):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    predictions = knn.predict(X_val_scaled)

    acc = accuracy_score(y_validate, predictions)

    k_values.append(k)
    accuracies.append(acc)

# Cari K terbaik
best_k = k_values[accuracies.index(max(accuracies))]
best_acc = max(accuracies)

print(f"K optimal = {best_k}")
print(f"Akurasi tertinggi = {best_acc:.4f} ({best_acc*100:.2f}%)")

# Plot sederhana (tanpa bintang)
plt.figure(figsize=(8, 5))
plt.plot(k_values, accuracies)
plt.axvline(best_k, linestyle='--', label=f'K optimal = {best_k}')
plt.xlabel('Nilai K')
plt.ylabel('Akurasi Validation')
plt.title('K Paling Optimal pada KNN')
plt.legend()
plt.grid(True)
plt.show()

"""### Classification Map"""

import time

start_time = time.time()
X_classify_scaled = scaler.transform(x_classify)
y_pred = knn.predict(X_classify_scaled)

knn_grid_classified_biome = [['Void' for _ in range(grid_x)] for _ in range(grid_y)]

for y in range(0, grid_y):
  for x in range(0, grid_x):
    knn_grid_classified_biome[y][x] = y_pred[y * grid_x + x]

end_time = time.time()
knn_elapsed_time = end_time - start_time
print("Classification Map Time: ", knn_elapsed_time)

"""### Biome Transition Report"""

from collections import Counter
print(knn_grid_classified_biome)

def get_neighbors(grid, y, x):
    neighbors = []

    # Loop melewati 8 cells yang bertetangga
    for dy in [-1, 0, 1]:
        for dx in [-1, 0, 1]:
            if dy == 0 and dx == 0:
                continue

            ny, nx = y + dy, x + dx

            if 0 <= ny < grid_y and 0 <= nx < grid_x:
              neighbors.append(str(grid[ny][nx]))

    return neighbors

counter_grid = [[None for _ in range(grid_x)] for _ in range(grid_y)]

for y in range(grid_y):
    for x in range(grid_x):
      neighbors = get_neighbors(knn_grid_classified_biome, y, x)
      counter_grid[y][x] = dict(Counter(neighbors))

print(penalty_lookup)
print(counter_grid)
penalty_grid = [[0 for _ in range(grid_x)] for _ in range(grid_y)]
penalty_average_grid = [[0 for _ in range(grid_x)] for _ in range(grid_y)]
penalty_total = 0
penalty_average_total = 0
for y in range(grid_y):
    for x in range(grid_x):
      total_neighbour = 0
      for biome, counter in counter_grid[y][x].items():
          classified_biome = knn_grid_classified_biome[y][x]
          penalty = counter * penalty_lookup[classified_biome][biome]
          total_neighbour += counter
          penalty_grid[y][x] += penalty
          penalty_total += penalty
      max_categorial_score = 2
      total_parameter_biome = len(biome_characteristics.items())
      average_score_divider = total_neighbour * max_categorial_score * total_parameter_biome
      penalty_average_grid[y][x] = penalty_grid[y][x] / average_score_divider
      penalty_average_total += penalty_average_grid[y][x]

print(penalty_grid)
print(penalty_average_grid)
print(len(biome_characteristics.items()))
print(penalty_total)
print(penalty_average_total)
print(f"Global Penalty Average: {penalty_average_total / (grid_x * grid_y) * 100}%")
print(f"Global Smoothness: {100 - (penalty_average_total / (grid_x * grid_y) * 100)}%")


# Morans I

biome_to_num = {b: i for i, b in enumerate(biome_characteristics.keys())}
numeric_grid = np.vectorize(lambda x: biome_to_num[x])(knn_grid_classified_biome)

def get_neighbors_coord(H, W, y, x):
    for ny in range(max(0, y-1), min(H, y+2)):
        for nx in range(max(0, x-1), min(W, x+2)):
            if (ny, nx) != (y, x):
                yield ny, nx

def compute_morans_I(numeric_grid):
    n = grid_y * grid_x
    values = numeric_grid.flatten()
    mean_val = values.mean()

    w_sum = 0
    num = 0
    denom = ((values - mean_val)**2).sum()

    for y in range(grid_y):
        for x in range(grid_x):
            v1 = numeric_grid[y][x]
            for ny, nx in get_neighbors_coord(grid_y, grid_x, y, x):
                v2 = numeric_grid[ny][nx]
                num += (v1 - mean_val) * (v2 - mean_val)
                w_sum += 1

    I = (n / w_sum) * (num / denom)
    return I

print(f"Morans I: {compute_morans_I(numeric_grid)}")


plt.imshow(penalty_average_grid, cmap="hot_r", interpolation="nearest")
plt.colorbar(label="Penalty Average")
plt.title("Biome Transition Smoothness Heatmap KNN")
plt.show()


"""## MLP

### Classification Report
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, f1_score
import time
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc  = le.transform(y_test)

X_train_scaled_mlp = X_train_scaled.astype(np.float64)
X_test_scaled_mlp  = X_test_scaled.astype(np.float64)

mlp = MLPClassifier(
    hidden_layer_sizes=(64, 64),
    learning_rate='adaptive',
    activation='relu',
    batch_size=256,
    max_iter=3000,
    early_stopping=True,
    random_state=42,
    alpha=1e-4,
    n_iter_no_change=40
)
mlp.fit(X_train_scaled_mlp, y_train_enc)

y_pred_mlp_enc = mlp.predict(X_test_scaled_mlp)
y_pred_mlp = le.inverse_transform(y_pred_mlp_enc)
print(len(y_pred_mlp))
print("Accuracy (MLP):", accuracy_score(y_test, y_pred_mlp))
print("Classification Report (MLP):\n")
print(classification_report(
    y_test,
    y_pred_mlp,
    digits=6
))

precision = precision_score(
    y_test,
    y_pred_mlp,
    average='weighted'
)

recall = recall_score(
    y_test,
    y_pred_mlp,
    average='weighted'
)

f1 = f1_score(
    y_test,
    y_pred_mlp,
    average='macro'
)

cm = confusion_matrix(y_test, y_pred_mlp)

print("Precision:", precision)
print("Recall:", recall)
print("F1:", f1)
print("Confusion Matrix:\n", cm)

start_time = time.time()
X_classify_scaled_mlp = scaler.transform(x_classify).astype(np.float64)
y_pred_mlp_full_enc = mlp.predict(X_classify_scaled_mlp)
y_pred_mlp_full = le.inverse_transform(y_pred_mlp_full_enc)

mlp_grid_classified_biome = [['Void' for _ in range(grid_x)] for _ in range(grid_y)]
for y in range(grid_y):
    for x in range(grid_x):
        mlp_grid_classified_biome[y][x] = y_pred_mlp_full[y * grid_x + x]

mlp_elapsed_time = time.time() - start_time
print("Training Time: ", mlp_elapsed_time)

"""### K-Fold"""

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import time

n_splits = 10
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
fold_no = 1
acc_scores = []

for train_index, test_index in kf.split(dataset_x):
    X_train_fold, X_test_fold = dataset_x.iloc[train_index], dataset_x.iloc[test_index]
    y_train_fold, y_test_fold = dataset_y.iloc[train_index], dataset_y.iloc[test_index]

    scaler_fold = StandardScaler()
    X_train_scaled_mlp = scaler_fold.fit_transform(X_train_fold).astype(np.float64)
    X_test_scaled_mlp  = scaler_fold.transform(X_test_fold).astype(np.float64)

    le = LabelEncoder()
    y_train_enc = le.fit_transform(y_train_fold)
    y_test_enc  = le.transform(y_test_fold)

    mlp = MLPClassifier(
        hidden_layer_sizes=(64, 64),
        learning_rate='adaptive',
        activation='relu',
        batch_size=256,
        max_iter=3000,
        early_stopping=True,
        random_state=42,
        alpha=1e-4,
        n_iter_no_change=40
    )
    mlp.fit(X_train_scaled_mlp, y_train_enc)

    y_pred_mlp_enc = mlp.predict(X_test_scaled_mlp)
    y_pred_mlp = le.inverse_transform(y_pred_mlp_enc)

    acc = accuracy_score(y_test_fold, y_pred_mlp)
    acc_scores.append(acc)

    print(f"Fold {fold_no}: Accuracy = {acc:.4f}")
    print(f"Classification Report Fold {fold_no}:\n", classification_report(y_test_fold, y_pred_mlp))
    fold_no += 1

print(f"\nAverage Accuracy: {np.mean(acc_scores):.4f}")

"""### Classification Map"""

start_time = time.time()
X_classify_scaled_mlp = scaler.transform(x_classify).astype(np.float64)
y_pred_mlp_full_enc = mlp.predict(X_classify_scaled_mlp)
y_pred_mlp_full = le.inverse_transform(y_pred_mlp_full_enc)

mlp_grid_classified_biome = [['Void' for _ in range(grid_x)] for _ in range(grid_y)]
for y in range(grid_y):
    for x in range(grid_x):
        mlp_grid_classified_biome[y][x] = y_pred_mlp_full[y * grid_x + x]

mlp_elapsed_time = time.time() - start_time
print("Classification Map Time: ", mlp_elapsed_time)

"""### Biome Transition Report"""

from collections import Counter
print(mlp_grid_classified_biome)

def get_neighbors(grid, y, x):
    neighbors = []

    # Loop melewati 8 cells yang bertetangga
    for dy in [-1, 0, 1]:
        for dx in [-1, 0, 1]:
            if dy == 0 and dx == 0:
                continue 

            ny, nx = y + dy, x + dx

            if 0 <= ny < grid_y and 0 <= nx < grid_x:
              neighbors.append(str(grid[ny][nx]))

    return neighbors

counter_grid = [[None for _ in range(grid_x)] for _ in range(grid_y)]

for y in range(grid_y):
    for x in range(grid_x):
      neighbors = get_neighbors(mlp_grid_classified_biome, y, x)
      counter_grid[y][x] = dict(Counter(neighbors))

print(penalty_lookup)
print(counter_grid)
penalty_grid = [[0 for _ in range(grid_x)] for _ in range(grid_y)]
penalty_average_grid = [[0 for _ in range(grid_x)] for _ in range(grid_y)]
penalty_total = 0
penalty_average_total = 0
for y in range(grid_y):
    for x in range(grid_x):
      total_neighbour = 0
      for biome, counter in counter_grid[y][x].items():
          classified_biome = mlp_grid_classified_biome[y][x]
          penalty = counter * penalty_lookup[classified_biome][biome]
          total_neighbour += counter
          penalty_grid[y][x] += penalty
          penalty_total += penalty
      max_categorial_score = 2
      total_parameter_biome = len(biome_characteristics.items())
      average_score_divider = total_neighbour * max_categorial_score * total_parameter_biome
      penalty_average_grid[y][x] = penalty_grid[y][x] / average_score_divider
      penalty_average_total += penalty_average_grid[y][x]

print(penalty_grid)
print(penalty_average_grid)
print(len(biome_characteristics.items()))
print(penalty_total)
print(penalty_average_total)
print(f"Global Penalty Average: {penalty_average_total / (grid_x * grid_y) * 100}%")
print(f"Global Smoothness: {100 - (penalty_average_total / (grid_x * grid_y) * 100)}%")


# Morans I

biome_to_num = {b: i for i, b in enumerate(biome_characteristics.keys())}
numeric_grid = np.vectorize(lambda x: biome_to_num[x])(mlp_grid_classified_biome)

def get_neighbors_coord(H, W, y, x):
    for ny in range(max(0, y-1), min(H, y+2)):
        for nx in range(max(0, x-1), min(W, x+2)):
            if (ny, nx) != (y, x):
                yield ny, nx

def compute_morans_I(numeric_grid):
    n = grid_y * grid_x
    values = numeric_grid.flatten()
    mean_val = values.mean()

    w_sum = 0
    num = 0
    denom = ((values - mean_val)**2).sum()

    for y in range(grid_y):
        for x in range(grid_x):
            v1 = numeric_grid[y][x]
            for ny, nx in get_neighbors_coord(grid_y, grid_x, y, x):
                v2 = numeric_grid[ny][nx]
                num += (v1 - mean_val) * (v2 - mean_val)
                w_sum += 1

    I = (n / w_sum) * (num / denom)
    return I

print(f"Morans I: {compute_morans_I(numeric_grid)}")


plt.imshow(penalty_average_grid, cmap="hot_r", interpolation="nearest")
plt.colorbar(label="Penalty Average")
plt.title("Biome Transition Smoothness Heatmap MLP")
plt.show()

"""# Map"""

import pandas as pd

print("Biome Distribution in Training Data:")
print(pd.Series(dataset_y).value_counts())

print(fuzzy_grid_classified_biome)
print(knn_grid_classified_biome)
print(mlp_grid_classified_biome)

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.lines as mlines

biome_colors = {
    'Rain Forest': '#0B3D2E',
    'Forest': '#1E6F3D',
    'Grassland': '#9ACD32',
    'Mesic': '#66CDAA',
    'Arid': '#C68642',
    'Desert': '#FAF0BE',
    'Void': '#F5F5F5'
}


biomes = list(biome_colors.keys())

biome_to_num = {biome: i for i, biome in enumerate(biomes)}

def generateMapFigure(title, grid_classified_biome, elapsed_time):
    grid_numeric = np.array([[biome_to_num[b] for b in row] for row in grid_classified_biome])

    flat = grid_numeric.flatten()
    counts = {biome: np.sum(flat == biome_to_num[biome]) for biome in biomes}
    total = flat.size
    percentages = {biome: count / total * 100 for biome, count in counts.items()}
    cmap = mcolors.ListedColormap([biome_colors[b] for b in biomes])
    norm = mcolors.BoundaryNorm(boundaries=np.arange(len(biomes) + 1) - 0.5, ncolors=len(biomes))
    plt.figure(figsize=(8, 8))
    plt.imshow(grid_numeric, cmap=cmap, norm=norm)
    plt.title(f"Biome Classification Map {title}")
    plt.axis('off')
    sorted_biomes = sorted(
        biomes,
        key=lambda b: percentages[b],
        reverse=True
    )
    handles = [plt.Line2D([0], [0], marker='s',
                      color=biome_colors[biome],
                      linestyle='', markersize=10,
                      label=f"{biome} ({percentages[biome]:.1f}%)")
           for biome in sorted_biomes]
    handles.append(mlines.Line2D([], [], color='white', label=f"Time elapsed: {elapsed_time:.2f}s"))

    plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.show()

generateMapFigure('Fuzzy', fuzzy_grid_classified_biome, fuzzy_elapsed_time)
generateMapFigure('KNN', knn_grid_classified_biome, knn_elapsed_time)
generateMapFigure('MLP', mlp_grid_classified_biome, mlp_elapsed_time)
